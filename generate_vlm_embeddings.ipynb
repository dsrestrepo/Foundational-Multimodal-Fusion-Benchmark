{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad3677ff",
   "metadata": {},
   "source": [
    "### Setup Environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71f0eb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datascience/conda/data_fusion_v0_0_1/lib/python3.8/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n",
      "2024-02-03 16:37:49.329156: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-03 16:37:49.366271: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-03 16:37:50.143088: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from src.vlm_models import CLIP, BLIP2, LLAVA\n",
    "from src.classifiers_base import preprocess_df\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be53111",
   "metadata": {},
   "source": [
    "## Embeddings Generation\n",
    "\n",
    "* **Dataframe:** Pandas dataframe with image path and text\n",
    "\n",
    "* **Image Column:** Column with the path to the images\n",
    "\n",
    "* **Text Column:** Column with text data\n",
    "\n",
    "* **Batch Size:** Integer with the size of the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5f5fad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'blip2'\n",
    "#model_name = 'clip'\n",
    "model_name = 'blip2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "880fc7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Instance of BLIP 2 model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2853b0eaebd4c96bac1a32c701d17d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if model_name.lower() == 'clip':\n",
    "    print('Creating Instance of CLIP model')\n",
    "    model = CLIP()\n",
    "elif model_name.lower() == 'blip2':\n",
    "    print('Creating Instance of BLIP 2 model')\n",
    "    model = BLIP2()\n",
    "elif model_name.lower() == 'llava':\n",
    "    print('Creating Instance of LLAVA model')\n",
    "    model = LLAVA()\n",
    "else:\n",
    "    raise NotImplementedError('The model should be clip, blip2 or llava')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdaedfa",
   "metadata": {},
   "source": [
    "## 1. DAQUAR\n",
    "\n",
    "* **[DAQUAR Dataset](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/vision-and-language/visual-turing-challenge#c7057)**:\n",
    "\n",
    "DAQUAR (Dataset for Question Answering on Real-world images) dataset was created for the purpose of advancing research in visual question answering (VQA). It consists of indoor scene images, each accompanied by sets of questions related to the scene's content. The dataset serves as a benchmark for training and evaluating models in understanding images and answering questions about them.\n",
    "\n",
    "We'll use the function `get_embeddings_df` to generate the embeddings in `datasets/daquar/images` and store the embeddings in `Embeddings/daquar/Embeddings_Backbone.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6175b21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12468/12468 [00:01<00:00, 9739.57it/s] \n",
      "100%|██████████| 12468/12468 [00:04<00:00, 2774.97it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e93c1557c64ce4be98f526a91b3b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/520 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "dataset = 'daquar'\n",
    "image_col = 'image_id'\n",
    "text_col = 'question'\n",
    "output_dir = f'Embeddings_vlm/{dataset}/'\n",
    "output_file = f'embeddings_{model_name}.csv'\n",
    "\n",
    "dataset_path = f'datasets/{dataset}/'\n",
    "images_dir = 'images/'\n",
    "labels = 'labels.csv'\n",
    "\n",
    "images_path = os.path.join(dataset_path, images_dir)\n",
    "labels_path = os.path.join(dataset_path, labels)\n",
    "\n",
    "df = preprocess_df(df=pd.read_csv(labels_path), image_columns=image_col, images_path=images_path)\n",
    "\n",
    "model.get_embeddings(dataframe=df, batch_size=batch_size, image_col_name=image_col, text_col_name=text_col, output_dir=output_dir, output_file=output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaced93",
   "metadata": {},
   "source": [
    "## 2. COCO-QA\n",
    "\n",
    "* **[COCO-QA Dataset](https://www.cs.toronto.edu/~mren/research/imageqa/data/cocoqa/)**:\n",
    "\n",
    "The COCO-QA (COCO Question-Answering) dataset is designed for the task of visual question-answering. It is a subset of the COCO (Common Objects in Context) dataset, which is a large-scale dataset containing images with object annotations. The COCO-QA dataset extends the COCO dataset by including questions and answers associated with the images. Each image in the COCO-QA dataset is accompanied by a set of questions and corresponding answers.\n",
    "\n",
    "We'll use the function `get_embeddings_df` to generate the embeddings in `datasets/coco-qa/images` and store the embeddings in `Embeddings/coco-qa/Embeddings_Backbone.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f8c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "dataset = 'coco-qa'\n",
    "image_col = 'image_id'\n",
    "text_col = 'questions'\n",
    "output_dir = f'Embeddings_vlm/{dataset}'\n",
    "output_file = 'embeddings_clip.csv'\n",
    "\n",
    "dataset_path = f'datasets/{dataset}/'\n",
    "images_dir = 'images/'\n",
    "labels = 'labels.csv'\n",
    "\n",
    "images_path = os.path.join(dataset_path, images_dir)\n",
    "labels_path = os.path.join(dataset_path, labels)\n",
    "\n",
    "df = preprocess_df(df=pd.read_csv(labels_path), image_columns=image_col, images_path=images_path)\n",
    "\n",
    "model.get_embeddings(dataframe=df, batch_size=batch_size, image_col_name=image_col, text_col_name=text_col, output_dir=output_dir, output_file=output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26facb8e",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b720686",
   "metadata": {},
   "source": [
    "## 2. Fakeddit\n",
    "\n",
    "* **[Fakeddit Dataset](https://fakeddit.netlify.app/)**:\n",
    "\n",
    "Fakeddit is a large-scale multimodal dataset for fine-grained fake news detection. It consists of over 1 million samples from multiple categories of fake news, including satire, misinformation, and fabricated news. The dataset includes text, images, metadata, and comment data, making it a rich resource for developing and evaluating fake news detection models.\n",
    "\n",
    "We'll use the function `get_embeddings_df` to generate the embeddings in `datasets/fakeddit/images` and store the embeddings in `Embeddings/fakeddit/Embeddings_Backbone.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5843568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "dataset = 'fakeddit'\n",
    "image_col = 'id'\n",
    "text_col = 'title'\n",
    "output_dir = f'Embeddings_vlm/{dataset}'\n",
    "output_file = 'embeddings_clip.csv'\n",
    "\n",
    "dataset_path = f'datasets/{dataset}/'\n",
    "images_dir = 'images/'\n",
    "labels = 'labels_subset.csv'\n",
    "\n",
    "images_path = os.path.join(dataset_path, images_dir)\n",
    "labels_path = os.path.join(dataset_path, labels)\n",
    "\n",
    "df = preprocess_df(df=pd.read_csv(labels_path), image_columns=image_col, images_path=images_path)\n",
    "\n",
    "model.get_embeddings(dataframe=df, batch_size=batch_size, image_col_name=image_col, text_col_name=text_col, output_dir=output_dir, output_file=output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0ce8bd",
   "metadata": {},
   "source": [
    "## 4. Recipes5k\n",
    "\n",
    "* **[Recipes5k Dataset](http://www.ub.edu/cvub/recipes5k/)**:\n",
    "\n",
    "The Recipes5k dataset comprises 4,826 recipes featuring images and corresponding ingredient lists, with 3,213 unique ingredients simplified from 1,014 by removing overly-descriptive particles, offering a diverse collection of alternative preparations for each of the 101 food types from Food101, meticulously balanced across training, validation, and test splits. The dataset addresses intra- and inter-class variability, extracted from Yummly with 50 recipes per food type.\n",
    "\n",
    "\n",
    "We'll use the function `get_embeddings_df` to generate the embeddings in `datasets/Recipes5k/images` and store the embeddings in `Embeddings/Recipes5k/Embeddings_Backbone.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17a669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "dataset = 'Recipes5k'\n",
    "image_col = 'image'\n",
    "text_col = 'ingredients'\n",
    "output_dir = f'Embeddings_vlm/{dataset}'\n",
    "output_file = 'embeddings_clip.csv'\n",
    "\n",
    "dataset_path = f'datasets/{dataset}/'\n",
    "images_dir = 'images/'\n",
    "labels = 'labels.csv'\n",
    "\n",
    "images_path = os.path.join(dataset_path, images_dir)\n",
    "labels_path = os.path.join(dataset_path, labels)\n",
    "\n",
    "df = preprocess_df(df=pd.read_csv(labels_path), image_columns=image_col, images_path=images_path)\n",
    "\n",
    "model.get_embeddings(dataframe=df, batch_size=batch_size, image_col_name=image_col, text_col_name=text_col, output_dir=output_dir, output_file=output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d180d9ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_fusion_v0_0_1]",
   "language": "python",
   "name": "conda-env-data_fusion_v0_0_1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
