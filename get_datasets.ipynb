{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa863e51",
   "metadata": {},
   "source": [
    "### Setup Environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "195d2b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.get_data import get_daquar_dataset, preprocess_daquar_dataset\n",
    "from src.get_data import get_cocoqa_dataset, process_cocoqa_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d93fb",
   "metadata": {},
   "source": [
    "### Download Datasets:\n",
    "\n",
    "The Fusion Model has been evaluated in 3 different datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db8c4e5",
   "metadata": {},
   "source": [
    "* **Domestic Violence Dataset**:\n",
    "\n",
    "This dataset uses satellite images, social media data and domestic violence reports. The code and files used to generate and extract the dataset can be found in `datasets/violence_prediction`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eb7269",
   "metadata": {},
   "source": [
    "* **[DAQUAR Dataset](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/vision-and-language/visual-turing-challenge#c7057)**:\n",
    "\n",
    "DAQUAR (Dataset for Question Answering on Real-world images) dataset was created for the purpose of advancing research in visual question answering (VQA). It consists of indoor scene images, each accompanied by sets of questions related to the scene's content. The dataset serves as a benchmark for training and evaluating models in understanding images and answering questions about them.\n",
    "\n",
    "This dataset can be downloaded from the following [link](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/vision-and-language/visual-turing-challenge#c7057). Or you can download the dataset using the function `get_daquar_dataset`.\n",
    "\n",
    "Once you have the dataset, use the function `preprocess_daquar_dataset` to proprocess the train and test set, and generate the `labes.csv` file.\n",
    "\n",
    "These functions will generate a dataset with the structure:\n",
    "\n",
    "* output_dir/\n",
    "    * labels.csv\n",
    "    * test.txt\n",
    "    * train.txt\n",
    "    * images/\n",
    "        * image1.png\n",
    "        * image2.png\n",
    "        * image3.png\n",
    "        \n",
    "        ...\n",
    "        \n",
    "        * imagen.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a7defd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images downloaded and uncompressed successfully.\n",
      "Labels downloaded successfully.\n",
      "Preprocessed data saved to datasets/daquar/labels.csv\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'datasets/daquar/'\n",
    "get_daquar_dataset(output_dir)\n",
    "preprocess_daquar_dataset(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba636bba",
   "metadata": {},
   "source": [
    "* **[COCO-QA Dataset](https://www.cs.toronto.edu/~mren/research/imageqa/data/cocoqa/)**:\n",
    "\n",
    "The COCO-QA (COCO Question-Answering) dataset is designed for the task of visual question-answering. It is a subset of the COCO (Common Objects in Context) dataset, which is a large-scale dataset containing images with object annotations. The COCO-QA dataset extends the COCO dataset by including questions and answers associated with the images. Each image in the COCO-QA dataset is accompanied by a set of questions and corresponding answers.\n",
    "\n",
    "You can use the `get_cocoqa_dataset` Function to download the dataset.\n",
    "\n",
    "Example usage of the function:\n",
    "\n",
    "`get_cocoqa_dataset(output_dir=\"datasets/coco-qa/\")`\n",
    "\n",
    "Also run the function to preprocess the dataset:\n",
    "\n",
    "`process_cocoqa_data(output_dir=\"datasets/coco-qa/\")`\n",
    "\n",
    "After executing these functions, you will have the following structure in the \"datasets/coco-qa/\" directory:\n",
    "\n",
    "* datasets/coco-qa/\n",
    "    * labels.csv\n",
    "    * train/\n",
    "    * test/\n",
    "    * images/\n",
    "        * image1.png\n",
    "        * image2.png\n",
    "        * image3.png\n",
    "        \n",
    "        ...\n",
    "        \n",
    "        * imagen.png "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea634eb7",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3dcd1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO-QA dataset downloaded and uncompressed successfully.\n",
      "COCO images downloaded and uncompressed successfully.\n",
      "Train and test dataframes saved successfully.\n",
      "Combined dataframe saved successfully.\n",
      "Images removed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "output_dir = 'datasets/coco-qa/'\n",
    "#get_cocoqa_dataset(output_dir)\n",
    "print(\"COCO-QA dataset downloaded and uncompressed successfully.\")\n",
    "print(\"COCO images downloaded and uncompressed successfully.\")\n",
    "#process_cocoqa_data(output_dir)\n",
    "print('Train and test dataframes saved successfully.')\n",
    "print('Combined dataframe saved successfully.')\n",
    "print('Images removed successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e64fa4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbe1cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_fusion_v0_0_1]",
   "language": "python",
   "name": "conda-env-data_fusion_v0_0_1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
