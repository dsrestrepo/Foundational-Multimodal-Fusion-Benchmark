{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "505a4e46",
   "metadata": {},
   "source": [
    "### Setup Environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ca34861",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/users/restrepoda/.conda/envs/base_ml/lib/python3.12/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#from src.embeddings import get_embeddings_df\n",
    "# from src.nlp_models_cpu import LLAMA\n",
    "from src.nlp_models_gpu import LLAMA\n",
    "from src.nlp_models_gpu import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "#from src.nlp_models import GPT\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eb44c5",
   "metadata": {},
   "source": [
    "## Embeddings Generation\n",
    "\n",
    "* **Batch Size:** Images per batch to convert to embeddings (Adjust depending on your memory)\n",
    "\n",
    "* **Path:** Path to the images\n",
    "\n",
    "* **Output Directory:** Directory to save the embeddings\n",
    "\n",
    "* **Backbone:** Select a backbone from the list of possible backbones:\n",
    "    * GPT-3.5 Turbo\n",
    "    * GPT 4\n",
    "    * LLAMA 2 7B\n",
    "    * LLAMA 2 13B\n",
    "    * LLAMA 2 70B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14abbb04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with 8-bit quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa4a876cc3c24b37abd7a03991c81775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3a15167339404ba730008cc81b70b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model (int8) is already on the correct device.\n",
      "Quantized model is on device: cuda:0\n",
      "Model: meta-llama/Llama-3.1-8B\n"
     ]
    }
   ],
   "source": [
    "# Choose your model from the list of models:\n",
    "#model - GPT()\n",
    "#model = LLAMA(embeddings=True, n_gpu_layers=400)\n",
    "#model_name='sentence-transformers/all-MiniLM-L6-v2'\n",
    "model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "quantization = 'int8'\n",
    "model = HuggingFaceEmbeddings(model_name=model_name, quantization=quantization, access_token='hf_...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48640f4",
   "metadata": {},
   "source": [
    "## 1. DAQUAR\n",
    "\n",
    "* **[DAQUAR Dataset](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/vision-and-language/visual-turing-challenge#c7057)**:\n",
    "\n",
    "DAQUAR (Dataset for Question Answering on Real-world images) dataset was created for the purpose of advancing research in visual question answering (VQA). It consists of indoor scene images, each accompanied by sets of questions related to the scene's content. The dataset serves as a benchmark for training and evaluating models in understanding images and answering questions about them.\n",
    "\n",
    "We'll use the method `get_embeddings_df` to generate the embeddings in `datasets/daquar/images` and store the embeddings in `Embeddings/daquar/Embeddings_Backbone.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f5c93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.path = 'datasets/daquar/labels.csv'\n",
    "column = 'question'\n",
    "directory = 'Embeddings/daquar'\n",
    "file = 'text_embeddings.csv'\n",
    "\n",
    "model.get_embedding_df(column, directory, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b25ad",
   "metadata": {},
   "source": [
    "## 2. COCO-QA\n",
    "\n",
    "* **[COCO-QA Dataset](https://www.cs.toronto.edu/~mren/research/imageqa/data/cocoqa/)**:\n",
    "\n",
    "The COCO-QA (COCO Question-Answering) dataset is designed for the task of visual question-answering. It is a subset of the COCO (Common Objects in Context) dataset, which is a large-scale dataset containing images with object annotations. The COCO-QA dataset extends the COCO dataset by including questions and answers associated with the images. Each image in the COCO-QA dataset is accompanied by a set of questions and corresponding answers.\n",
    "\n",
    "We'll use the function `get_embeddings_df` to generate the embeddings in `datasets/coco-qa/images` and store the embeddings in `Embeddings/coco-qa/Embeddings_Backbone.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb84d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.path = 'datasets/coco-qa/labels.csv'\n",
    "column = 'questions'\n",
    "directory = 'Embeddings/coco-qa'\n",
    "file = 'text_embeddings.csv'\n",
    "\n",
    "model.get_embedding_df(column, directory, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec9e918",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ae2c41",
   "metadata": {},
   "source": [
    "## 3. Fakeddit\n",
    "\n",
    "* **[Fakeddit Dataset](https://fakeddit.netlify.app/)**:\n",
    "\n",
    "Fakeddit is a large-scale multimodal dataset for fine-grained fake news detection. It consists of over 1 million samples from multiple categories of fake news, including satire, misinformation, and fabricated news. The dataset includes text, images, metadata, and comment data, making it a rich resource for developing and evaluating fake news detection models.\n",
    "\n",
    "We'll use the function `get_embeddings_df` to generate the embeddings in `datasets/fakeddit/images` and store the embeddings in `Embeddings/fakeddit/Embeddings_Backbone.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f92cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Embeddings generated!\n",
      "5000 Embeddings generated!\n",
      "10000 Embeddings generated!\n"
     ]
    }
   ],
   "source": [
    "model.path = 'datasets/fakeddit/labels.csv'\n",
    "column = 'title'\n",
    "directory = 'Embeddings/fakeddit'\n",
    "file = 'text_embeddings.csv'\n",
    "\n",
    "model.get_embedding_df(column, directory, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd2fa34",
   "metadata": {},
   "source": [
    "## 4. Recipes5k\n",
    "\n",
    "* **[Recipes5k Dataset](http://www.ub.edu/cvub/recipes5k/)**:\n",
    "\n",
    "The Recipes5k dataset comprises 4,826 recipes featuring images and corresponding ingredient lists, with 3,213 unique ingredients simplified from 1,014 by removing overly-descriptive particles, offering a diverse collection of alternative preparations for each of the 101 food types from Food101, meticulously balanced across training, validation, and test splits. The dataset addresses intra- and inter-class variability, extracted from Yummly with 50 recipes per food type.\n",
    "\n",
    "\n",
    "We'll use the function `get_embeddings_df` to generate the embeddings in `datasets/Recipes5k/images` and store the embeddings in `Embeddings/Recipes5k/Embeddings_Backbone.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28639868",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.path = 'datasets/Recipes5k/labels.csv'\n",
    "column = 'ingredients'\n",
    "directory = 'Embeddings/Recipes5k'\n",
    "file = 'text_embeddings.csv'\n",
    "\n",
    "model.get_embedding_df(column, directory, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6245a7f2",
   "metadata": {},
   "source": [
    "## 5. BRSET\n",
    "* **[BRSET Dataset](https://physionet.org/content/brazilian-ophthalmological/1.0.0/)**:\n",
    "\n",
    "The Brazilian Multilabel Ophthalmological Dataset (BRSET) stands as a pioneering initiative aimed at bridging the gap in ophthalmological datasets, particularly for under-represented populations in low and medium-income countries. This comprehensive dataset encompasses 16,266 images from 8,524 Brazilian patients, incorporating a wide array of data points including demographics, anatomical parameters of the macula, optic disc, and vessels, along with quality control metrics such as focus, illumination, image field, and artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac97289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.path = '/gpfs/workdir/restrepoda/datasets/BRSET/brset/labels.csv'\n",
    "column = 'text'\n",
    "directory = 'Embeddings/brset'\n",
    "file = 'text_embeddings_llama31_8b.csv'\n",
    "#file = 'text_embeddings_minilm.csv'\n",
    "\n",
    "model.get_embedding_df(column, directory, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61459f23-1fb0-4981-a5c2-24f2da4e593b",
   "metadata": {},
   "source": [
    "## 5. mBRSET\n",
    "* **[mBRSET Dataset](https://physionet.org/content/mbrset/1.0/)**:\n",
    "\n",
    "The Mobile Brazilian Multilabel Ophthalmological Dataset (mBRSET) stands as a pioneering initiative aimed at bridging the gap in ophthalmological datasets using mobile cameras, particularly for under-represented populations in low and medium-income countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4072c2-a440-4e75-8f00-9c50a0de1c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.path = '/gpfs/workdir/restrepoda/datasets/mBRSET/mbrset/labels.csv'\n",
    "column = 'text'\n",
    "directory = 'Embeddings/mbrset'\n",
    "file = 'text_embeddings_llama31_8b.csv'\n",
    "#file = 'text_embeddings_minilm.csv'\n",
    "\n",
    "model.get_embedding_df(column, directory, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9312e6d1",
   "metadata": {},
   "source": [
    "### 6. HAM10000 dataset\n",
    "\n",
    "* [HAM10000 dataset](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T)\n",
    "\n",
    "The MNIST: HAM10000 dataset is a large collection of dermatoscopic images from different populations, acquired and stored by the Department of Dermatology at the Medical University of Vienna, Austria. It consists of 10,015 dermatoscopic images which can serve as a training set for academic machine learning purposes in tasks like skin lesion analysis and classification, specifically focusing on the detection of melanoma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45aaf76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Embeddings generated!\n",
      "500 Embeddings generated!\n",
      "1000 Embeddings generated!\n",
      "1500 Embeddings generated!\n",
      "2000 Embeddings generated!\n",
      "2500 Embeddings generated!\n",
      "3000 Embeddings generated!\n",
      "3500 Embeddings generated!\n",
      "4000 Embeddings generated!\n",
      "4500 Embeddings generated!\n",
      "5000 Embeddings generated!\n",
      "5500 Embeddings generated!\n",
      "6000 Embeddings generated!\n",
      "6500 Embeddings generated!\n",
      "7000 Embeddings generated!\n",
      "7500 Embeddings generated!\n",
      "8000 Embeddings generated!\n",
      "8500 Embeddings generated!\n",
      "9000 Embeddings generated!\n",
      "9500 Embeddings generated!\n",
      "10000 Embeddings generated!\n"
     ]
    }
   ],
   "source": [
    "model.path = 'datasets/ham10000/labels.csv'\n",
    "column = 'text'\n",
    "directory = 'Embeddings/ham10000'\n",
    "file = 'text_embeddings.csv'\n",
    "\n",
    "model.get_embedding_df(column, directory, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1e2ea2",
   "metadata": {},
   "source": [
    "## 7. Colombian Multimodal Satellite dataset\n",
    "* **[A Multi-Modal Satellite Imagery Dataset for Public Health Analysis in Colombia](https://physionet.org/content/multimodal-satellite-data/1.0.0/)**:\n",
    "\n",
    "The Multi-Modal Satellite Imagery Dataset in Colombia integrates economic, demographic, meteorological, and epidemiological data. It comprises 12,636 high-quality satellite images from 81 municipalities between 2016 and 2018, with minimal cloud cover. Its applications include deforestation monitoring, education indices forecasting, water quality assessment, extreme climatic event tracking, epidemic illness addressing, and precision agriculture optimization. We'll use it shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e034f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Embeddings generated!\n",
      "500 Embeddings generated!\n",
      "1000 Embeddings generated!\n",
      "1500 Embeddings generated!\n"
     ]
    }
   ],
   "source": [
    "model.path = 'datasets/satellitedata/labels.csv'\n",
    "column = 'text'\n",
    "directory = 'Embeddings/satellitedata'\n",
    "file = 'text_embeddings.csv'\n",
    "\n",
    "model.get_embedding_df(column, directory, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba7401c",
   "metadata": {},
   "source": [
    "## 8. MIMIC CXR\n",
    "* **[MIMIC CXR](https://physionet.org/content/mimic-cxr/2.0.0/#files-panel)**:\n",
    "\n",
    "The MIMIC-CXR (Medical Information Mart for Intensive Care, Chest X-Ray) dataset is a large, publicly available collection of chest radiographs with associated radiology reports. It was developed by the MIT Lab for Computational Physiology and provides an extensive resource for training and evaluating machine learning models in the field of medical imaging, particularly in automated radiograph interpretation and natural language processing for clinical narratives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b569c8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Embeddings generated!\n"
     ]
    }
   ],
   "source": [
    "model.path = 'datasets/mimic/labels.csv'\n",
    "column = 'text'\n",
    "directory = 'Embeddings/mimic'\n",
    "file = 'text_embeddings.csv'\n",
    "\n",
    "model.get_embedding_df(column, directory, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1232da58",
   "metadata": {},
   "source": [
    "# 9. Joslin Center Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4201aea9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Embeddings generated!\n"
     ]
    }
   ],
   "source": [
    "model.path = 'datasets/joslin/labels.csv'\n",
    "column = 'text'\n",
    "directory = 'Embeddings/joslin'\n",
    "file = 'text_embeddings.csv'\n",
    "\n",
    "model.get_embedding_df(column, directory, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
