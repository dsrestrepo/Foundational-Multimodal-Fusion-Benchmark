{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3394abea",
   "metadata": {},
   "source": [
    "### Setup Environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ea3791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.embeddings import get_embeddings_df\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3335c7a3",
   "metadata": {},
   "source": [
    "## Embeddings Generation\n",
    "\n",
    "* **Batch Size:** Images per batch to convert to embeddings (Adjust depending on your memory)\n",
    "\n",
    "* **Path:** Path to the images\n",
    "\n",
    "* **Output Directory:** Directory to save the embeddings\n",
    "\n",
    "* **Backbone:** Select a backbone from the list of possible backbones:\n",
    "    * 'dinov2_small'\n",
    "    * 'dinov2_base'\n",
    "    * 'dinov2_large'\n",
    "    * 'dinov2_giant'\n",
    "    * 'sam_base'\n",
    "    * 'sam_large'\n",
    "    * 'sam_huge'\n",
    "    * 'clip_base',\n",
    "    * 'clip_large',\n",
    "    * 'convnextv2_tiny'\n",
    "    * 'convnextv2_base'\n",
    "    * 'convnextv2_large'\n",
    "    * 'convnext_tiny'\n",
    "    * 'convnext_small'\n",
    "    * 'convnext_base'\n",
    "    * 'convnext_large'\n",
    "    * 'swin_tiny'\n",
    "    * 'swin_small'\n",
    "    * 'swin_base'\n",
    "    * 'vit_base'\n",
    "    * 'vit_large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46e65086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dinov2_small',\n",
       " 'dinov2_base',\n",
       " 'dinov2_large',\n",
       " 'dinov2_giant',\n",
       " 'clip_base',\n",
       " 'clip_large',\n",
       " 'sam_base',\n",
       " 'sam_large',\n",
       " 'sam_huge',\n",
       " 'convnextv2_tiny',\n",
       " 'convnextv2_base',\n",
       " 'convnextv2_large',\n",
       " 'convnext_tiny',\n",
       " 'convnext_small',\n",
       " 'convnext_base',\n",
       " 'convnext_large',\n",
       " 'swin_tiny',\n",
       " 'swin_small',\n",
       " 'swin_base',\n",
       " 'vit_base',\n",
       " 'vit_large']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Foundational Models\n",
    "dino_backbone = ['dinov2_small', 'dinov2_base', 'dinov2_large', 'dinov2_giant']\n",
    "\n",
    "sam_backbone = ['sam_base', 'sam_large', 'sam_huge']\n",
    "\n",
    "clip_backbone = ['clip_base', 'clip_large']\n",
    "\n",
    "# ImageNet:\n",
    "\n",
    "### Convnext\n",
    "convnext_backbone = ['convnextv2_tiny', 'convnextv2_base', 'convnextv2_large'] + ['convnext_tiny', 'convnext_small', 'convnext_base', 'convnext_large']\n",
    "\n",
    "### Swin Transformer\n",
    "swin_transformer_backbone = ['swin_tiny', 'swin_small', 'swin_base']\n",
    "\n",
    "### ViT\n",
    "vit_backbone = ['vit_base', 'vit_large']\n",
    "\n",
    "backbones = dino_backbone + clip_backbone + sam_backbone + convnext_backbone + swin_transformer_backbone + vit_backbone\n",
    "\n",
    "backbones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5804de8",
   "metadata": {},
   "source": [
    "### Satellite Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9ff12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360be0af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d38350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0862677",
   "metadata": {},
   "source": [
    "* **[DAQUAR Dataset](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/vision-and-language/visual-turing-challenge#c7057)**:\n",
    "\n",
    "DAQUAR (Dataset for Question Answering on Real-world images) dataset was created for the purpose of advancing research in visual question answering (VQA). It consists of indoor scene images, each accompanied by sets of questions related to the scene's content. The dataset serves as a benchmark for training and evaluating models in understanding images and answering questions about them.\n",
    "\n",
    "We'll use the function `get_embeddings_df` to generate the embeddings in `datasets/daquar/images` and store the embeddings in `Embeddings/daquar/Embeddings_Backbone.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55bfe48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################  dinov2_base  ##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/datascience/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch number: 10\n",
      "Processed batch number: 20\n",
      "Processed batch number: 30\n",
      "Processed batch number: 40\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "path = 'datasets/daquar/images'\n",
    "dataset = 'daquar'\n",
    "backbone = 'dinov2_base'\n",
    "out_dir = 'Embeddings'\n",
    "\n",
    "get_embeddings_df(batch_size=batch_size, path=path, dataset_name=dataset, backbone=backbone, directory=out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d2b33d",
   "metadata": {},
   "source": [
    "* **[COCO-QA Dataset](https://www.cs.toronto.edu/~mren/research/imageqa/data/cocoqa/)**:\n",
    "\n",
    "The COCO-QA (COCO Question-Answering) dataset is designed for the task of visual question-answering. It is a subset of the COCO (Common Objects in Context) dataset, which is a large-scale dataset containing images with object annotations. The COCO-QA dataset extends the COCO dataset by including questions and answers associated with the images. Each image in the COCO-QA dataset is accompanied by a set of questions and corresponding answers.\n",
    "\n",
    "We'll use the function `get_embeddings_df` to generate the embeddings in `datasets/coco-qa/images` and store the embeddings in `Embeddings/coco-qa/Embeddings_Backbone.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f3644ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################  dinov2_base  ##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/datascience/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch number: 10\n",
      "Processed batch number: 20\n",
      "Processed batch number: 30\n",
      "Processed batch number: 40\n",
      "Processed batch number: 50\n",
      "Processed batch number: 60\n",
      "Processed batch number: 70\n",
      "Processed batch number: 80\n",
      "Processed batch number: 90\n",
      "Processed batch number: 100\n",
      "Processed batch number: 110\n",
      "Processed batch number: 120\n",
      "Processed batch number: 130\n",
      "Processed batch number: 140\n",
      "Processed batch number: 150\n",
      "Processed batch number: 160\n",
      "Processed batch number: 170\n",
      "Processed batch number: 180\n",
      "Processed batch number: 190\n",
      "Processed batch number: 200\n",
      "Processed batch number: 210\n",
      "Processed batch number: 220\n",
      "Processed batch number: 230\n",
      "Processed batch number: 240\n",
      "Processed batch number: 250\n",
      "Processed batch number: 260\n",
      "Processed batch number: 270\n",
      "Processed batch number: 280\n",
      "Processed batch number: 290\n",
      "Processed batch number: 300\n",
      "Processed batch number: 310\n",
      "Processed batch number: 320\n",
      "Processed batch number: 330\n",
      "Processed batch number: 340\n",
      "Processed batch number: 350\n",
      "Processed batch number: 360\n",
      "Processed batch number: 370\n",
      "Processed batch number: 380\n",
      "Processed batch number: 390\n",
      "Processed batch number: 400\n",
      "Processed batch number: 410\n",
      "Processed batch number: 420\n",
      "Processed batch number: 430\n",
      "Processed batch number: 440\n",
      "Processed batch number: 450\n",
      "Processed batch number: 460\n",
      "Processed batch number: 470\n",
      "Processed batch number: 480\n",
      "Processed batch number: 490\n",
      "Processed batch number: 500\n",
      "Processed batch number: 510\n",
      "Processed batch number: 520\n",
      "Processed batch number: 530\n",
      "Processed batch number: 540\n",
      "Processed batch number: 550\n",
      "Processed batch number: 560\n",
      "Processed batch number: 570\n",
      "Processed batch number: 580\n",
      "Processed batch number: 590\n",
      "Processed batch number: 600\n",
      "Processed batch number: 610\n",
      "Processed batch number: 620\n",
      "Processed batch number: 630\n",
      "Processed batch number: 640\n",
      "Processed batch number: 650\n",
      "Processed batch number: 660\n",
      "Processed batch number: 670\n",
      "Processed batch number: 680\n",
      "Processed batch number: 690\n",
      "Processed batch number: 700\n",
      "Processed batch number: 710\n",
      "Processed batch number: 720\n",
      "Processed batch number: 730\n",
      "Processed batch number: 740\n",
      "Processed batch number: 750\n",
      "Processed batch number: 760\n",
      "Processed batch number: 770\n",
      "Processed batch number: 780\n",
      "Processed batch number: 790\n",
      "Processed batch number: 800\n",
      "Processed batch number: 810\n",
      "Processed batch number: 820\n",
      "Processed batch number: 830\n",
      "Processed batch number: 840\n",
      "Processed batch number: 850\n",
      "Processed batch number: 860\n",
      "Processed batch number: 870\n",
      "Processed batch number: 880\n",
      "Processed batch number: 890\n",
      "Processed batch number: 900\n",
      "Processed batch number: 910\n",
      "Processed batch number: 920\n",
      "Processed batch number: 930\n",
      "Processed batch number: 940\n",
      "Processed batch number: 950\n",
      "Processed batch number: 960\n",
      "Processed batch number: 970\n",
      "Processed batch number: 980\n",
      "Processed batch number: 990\n",
      "Processed batch number: 1000\n",
      "Processed batch number: 1010\n",
      "Processed batch number: 1020\n",
      "Processed batch number: 1030\n",
      "Processed batch number: 1040\n",
      "Processed batch number: 1050\n",
      "Processed batch number: 1060\n",
      "Processed batch number: 1070\n",
      "Processed batch number: 1080\n",
      "Processed batch number: 1090\n",
      "Processed batch number: 1100\n",
      "Processed batch number: 1110\n",
      "Processed batch number: 1120\n",
      "Processed batch number: 1130\n",
      "Processed batch number: 1140\n",
      "Processed batch number: 1150\n",
      "Processed batch number: 1160\n",
      "Processed batch number: 1170\n",
      "Processed batch number: 1180\n",
      "Processed batch number: 1190\n",
      "Processed batch number: 1200\n",
      "Processed batch number: 1210\n",
      "Processed batch number: 1220\n",
      "Processed batch number: 1230\n",
      "Processed batch number: 1240\n",
      "Processed batch number: 1250\n",
      "Processed batch number: 1260\n",
      "Processed batch number: 1270\n",
      "Processed batch number: 1280\n",
      "Processed batch number: 1290\n",
      "Processed batch number: 1300\n",
      "Processed batch number: 1310\n",
      "Processed batch number: 1320\n",
      "Processed batch number: 1330\n",
      "Processed batch number: 1340\n",
      "Processed batch number: 1350\n",
      "Processed batch number: 1360\n",
      "Processed batch number: 1370\n",
      "Processed batch number: 1380\n",
      "Processed batch number: 1390\n",
      "Processed batch number: 1400\n",
      "Processed batch number: 1410\n",
      "Processed batch number: 1420\n",
      "Processed batch number: 1430\n",
      "Processed batch number: 1440\n",
      "Processed batch number: 1450\n",
      "Processed batch number: 1460\n",
      "Processed batch number: 1470\n",
      "Processed batch number: 1480\n",
      "Processed batch number: 1490\n",
      "Processed batch number: 1500\n",
      "Processed batch number: 1510\n",
      "Processed batch number: 1520\n",
      "Processed batch number: 1530\n",
      "Processed batch number: 1540\n",
      "Processed batch number: 1550\n",
      "Processed batch number: 1560\n",
      "Processed batch number: 1570\n",
      "Processed batch number: 1580\n",
      "Processed batch number: 1590\n",
      "Processed batch number: 1600\n",
      "Processed batch number: 1610\n",
      "Processed batch number: 1620\n",
      "Processed batch number: 1630\n",
      "Processed batch number: 1640\n",
      "Processed batch number: 1650\n",
      "Processed batch number: 1660\n",
      "Processed batch number: 1670\n",
      "Processed batch number: 1680\n",
      "Processed batch number: 1690\n",
      "Processed batch number: 1700\n",
      "Processed batch number: 1710\n",
      "Processed batch number: 1720\n",
      "Processed batch number: 1730\n",
      "Processed batch number: 1740\n",
      "Processed batch number: 1750\n",
      "Processed batch number: 1760\n",
      "Processed batch number: 1770\n",
      "Processed batch number: 1780\n",
      "Processed batch number: 1790\n",
      "Processed batch number: 1800\n",
      "Processed batch number: 1810\n",
      "Processed batch number: 1820\n",
      "Processed batch number: 1830\n",
      "Processed batch number: 1840\n",
      "Processed batch number: 1850\n",
      "Processed batch number: 1860\n",
      "Processed batch number: 1870\n",
      "Processed batch number: 1880\n",
      "Processed batch number: 1890\n",
      "Processed batch number: 1900\n",
      "Processed batch number: 1910\n",
      "Processed batch number: 1920\n",
      "Processed batch number: 1930\n",
      "Processed batch number: 1940\n",
      "Processed batch number: 1950\n",
      "Processed batch number: 1960\n",
      "Processed batch number: 1970\n",
      "Processed batch number: 1980\n",
      "Processed batch number: 1990\n",
      "Processed batch number: 2000\n",
      "Processed batch number: 2010\n",
      "Processed batch number: 2020\n",
      "Processed batch number: 2030\n",
      "Processed batch number: 2040\n",
      "Processed batch number: 2050\n",
      "Processed batch number: 2060\n",
      "Processed batch number: 2070\n",
      "Processed batch number: 2080\n",
      "Processed batch number: 2090\n",
      "Processed batch number: 2100\n",
      "Processed batch number: 2110\n",
      "Processed batch number: 2120\n",
      "Processed batch number: 2130\n",
      "Processed batch number: 2140\n",
      "Processed batch number: 2150\n",
      "Processed batch number: 2160\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "path = 'datasets/coco-qa/images'\n",
    "dataset = 'coco-qa'\n",
    "backbone = 'dinov2_base'\n",
    "out_dir = 'Embeddings'\n",
    "\n",
    "get_embeddings_df(batch_size=batch_size, path=path, dataset_name=dataset, backbone=backbone, directory=out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b50b0",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faf5859",
   "metadata": {},
   "source": [
    "* **[Fakeddit Dataset](https://fakeddit.netlify.app/)**:\n",
    "\n",
    "Fakeddit is a large-scale multimodal dataset for fine-grained fake news detection. It consists of over 1 million samples from multiple categories of fake news, including satire, misinformation, and fabricated news. The dataset includes text, images, metadata, and comment data, making it a rich resource for developing and evaluating fake news detection models.\n",
    "\n",
    "We'll use the function `get_embeddings_df` to generate the embeddings in `datasets/fakeddit/images` and store the embeddings in `Embeddings/fakeddit/Embeddings_Backbone.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1788875e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################  dinov2_base  ##################################################\n",
      "Skipping c82xgjo.jpg due to error\n",
      "Skipping cw74890.jpg due to error\n",
      "Skipping cal56d3.jpg due to error\n",
      "Skipping cqpkb27.jpg due to error\n",
      "Skipping crzhesn.jpg due to error\n",
      "Skipping ca9ko4d.jpg due to error\n",
      "Skipping cjrbguf.jpg due to error\n",
      "Skipping cmcpahb.jpg due to error\n",
      "Skipping eqgno1g.jpg due to error\n",
      "Skipping ck5acj4.jpg due to error\n",
      "Skipping cxmsg1b.jpg due to error\n",
      "Skipping ch6n9gp.jpg due to error\n",
      "Skipping ddlrl9s.jpg due to error\n",
      "Skipping c5qt713.jpg due to error\n",
      "Skipping csjp1ca.jpg due to error\n",
      "Skipping caoxh2h.jpg due to error\n",
      "Skipping dkiszvg.jpg due to error\n",
      "Skipping ckzrdnd.jpg due to error\n",
      "Skipping dkuuac9.jpg due to error\n",
      "Skipping c8mt6cw.jpg due to error\n",
      "Skipping c88qgu5.jpg due to error\n",
      "Skipping c8i3it0.jpg due to error\n",
      "Skipping dim4v3i.jpg due to error\n",
      "Skipping eti8jc5.jpg due to error\n",
      "Skipping dga8gnv.jpg due to error\n",
      "Skipping c89lshc.jpg due to error\n",
      "Skipping cbotfac.jpg due to error\n",
      "Skipping cjllu0x.jpg due to error\n",
      "Skipping cortykv.jpg due to error\n",
      "Skipping d1ulk01.jpg due to error\n",
      "Skipping dpmo2kx.jpg due to error\n",
      "Skipping cgtgnmv.jpg due to error\n",
      "Skipping c9423e2.jpg due to error\n",
      "Skipping ca47gdr.jpg due to error\n",
      "Skipping ekr7yxq.jpg due to error\n",
      "Skipping cfcp62e.jpg due to error\n",
      "Skipping cfw3u5c.jpg due to error\n",
      "Skipping caj1um2.jpg due to error\n",
      "Skipping cejto4u.jpg due to error\n",
      "Skipping ch1pcor.jpg due to error\n",
      "Skipping cjnb91u.jpg due to error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/datascience/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch number: 10\n",
      "Processed batch number: 20\n",
      "Processed batch number: 30\n",
      "Processed batch number: 40\n",
      "Processed batch number: 50\n",
      "Processed batch number: 60\n",
      "Processed batch number: 70\n",
      "Processed batch number: 80\n",
      "Processed batch number: 90\n",
      "Processed batch number: 100\n",
      "Processed batch number: 110\n",
      "Processed batch number: 120\n",
      "Processed batch number: 130\n",
      "Processed batch number: 140\n",
      "Processed batch number: 150\n",
      "Processed batch number: 160\n",
      "Processed batch number: 170\n",
      "Processed batch number: 180\n",
      "Processed batch number: 190\n",
      "Processed batch number: 200\n",
      "Processed batch number: 210\n",
      "Processed batch number: 220\n",
      "Processed batch number: 230\n",
      "Processed batch number: 240\n",
      "Processed batch number: 250\n",
      "Processed batch number: 260\n",
      "Processed batch number: 270\n",
      "Processed batch number: 280\n",
      "Processed batch number: 290\n",
      "Processed batch number: 300\n",
      "Processed batch number: 310\n",
      "Processed batch number: 320\n",
      "Processed batch number: 330\n",
      "Processed batch number: 340\n",
      "Processed batch number: 350\n",
      "Processed batch number: 360\n",
      "Processed batch number: 370\n",
      "Processed batch number: 380\n",
      "Processed batch number: 390\n",
      "Processed batch number: 400\n",
      "Processed batch number: 410\n",
      "Processed batch number: 420\n",
      "Processed batch number: 430\n",
      "Processed batch number: 440\n",
      "Processed batch number: 450\n",
      "Processed batch number: 460\n",
      "Processed batch number: 470\n",
      "Processed batch number: 480\n",
      "Processed batch number: 490\n",
      "Processed batch number: 500\n",
      "Processed batch number: 510\n",
      "Processed batch number: 520\n",
      "Processed batch number: 530\n",
      "Processed batch number: 540\n",
      "Processed batch number: 550\n",
      "Processed batch number: 560\n",
      "Processed batch number: 570\n",
      "Processed batch number: 580\n",
      "Processed batch number: 590\n",
      "Processed batch number: 600\n",
      "Processed batch number: 610\n",
      "Processed batch number: 620\n",
      "Processed batch number: 630\n",
      "Processed batch number: 640\n",
      "Processed batch number: 650\n",
      "Processed batch number: 660\n",
      "Processed batch number: 670\n",
      "Processed batch number: 680\n",
      "Processed batch number: 690\n",
      "Processed batch number: 700\n",
      "Processed batch number: 710\n",
      "Processed batch number: 720\n",
      "Processed batch number: 730\n",
      "Processed batch number: 740\n",
      "Processed batch number: 750\n",
      "Processed batch number: 760\n",
      "Processed batch number: 770\n",
      "Processed batch number: 780\n",
      "Processed batch number: 790\n",
      "Processed batch number: 800\n",
      "Processed batch number: 810\n",
      "Processed batch number: 820\n",
      "Processed batch number: 830\n",
      "Processed batch number: 840\n",
      "Processed batch number: 850\n",
      "Processed batch number: 860\n",
      "Processed batch number: 870\n",
      "Processed batch number: 880\n",
      "Processed batch number: 890\n",
      "Processed batch number: 900\n",
      "Processed batch number: 910\n",
      "Processed batch number: 920\n",
      "Processed batch number: 930\n",
      "Processed batch number: 940\n",
      "Processed batch number: 950\n",
      "Processed batch number: 960\n",
      "Processed batch number: 970\n",
      "Processed batch number: 980\n",
      "Processed batch number: 990\n",
      "Processed batch number: 1000\n",
      "Processed batch number: 1010\n",
      "Processed batch number: 1020\n",
      "Processed batch number: 1030\n",
      "Processed batch number: 1040\n",
      "Processed batch number: 1050\n",
      "Processed batch number: 1060\n",
      "Processed batch number: 1070\n",
      "Processed batch number: 1080\n",
      "Processed batch number: 1090\n",
      "Processed batch number: 1100\n",
      "Processed batch number: 1110\n",
      "Processed batch number: 1120\n",
      "Processed batch number: 1130\n",
      "Processed batch number: 1140\n",
      "Processed batch number: 1150\n",
      "Processed batch number: 1160\n",
      "Processed batch number: 1170\n",
      "Processed batch number: 1180\n",
      "Processed batch number: 1190\n",
      "Processed batch number: 1200\n",
      "Processed batch number: 1210\n",
      "Processed batch number: 1220\n",
      "Processed batch number: 1230\n",
      "Processed batch number: 1240\n",
      "Processed batch number: 1250\n",
      "Processed batch number: 1260\n",
      "Processed batch number: 1270\n",
      "Processed batch number: 1280\n",
      "Processed batch number: 1290\n",
      "Processed batch number: 1300\n",
      "Processed batch number: 1310\n",
      "Processed batch number: 1320\n",
      "Processed batch number: 1330\n",
      "Processed batch number: 1340\n",
      "Processed batch number: 1350\n",
      "Processed batch number: 1360\n",
      "Processed batch number: 1370\n",
      "Processed batch number: 1380\n",
      "Processed batch number: 1390\n",
      "Processed batch number: 1400\n",
      "Processed batch number: 1410\n",
      "Processed batch number: 1420\n",
      "Processed batch number: 1430\n",
      "Processed batch number: 1440\n",
      "Processed batch number: 1450\n",
      "Processed batch number: 1460\n",
      "Processed batch number: 1470\n",
      "Processed batch number: 1480\n",
      "Processed batch number: 1490\n",
      "Processed batch number: 1500\n",
      "Processed batch number: 1510\n",
      "Processed batch number: 1520\n",
      "Processed batch number: 1530\n",
      "Processed batch number: 1540\n",
      "Processed batch number: 1550\n",
      "Processed batch number: 1560\n",
      "Processed batch number: 1570\n",
      "Processed batch number: 1580\n",
      "Processed batch number: 1590\n",
      "Processed batch number: 1600\n",
      "Processed batch number: 1610\n",
      "Processed batch number: 1620\n",
      "Processed batch number: 1630\n",
      "Processed batch number: 1640\n",
      "Processed batch number: 1650\n",
      "Processed batch number: 1660\n",
      "Processed batch number: 1670\n",
      "Processed batch number: 1680\n",
      "Processed batch number: 1690\n",
      "Processed batch number: 1700\n",
      "Processed batch number: 1710\n",
      "Processed batch number: 1720\n",
      "Processed batch number: 1730\n",
      "Processed batch number: 1740\n",
      "Processed batch number: 1750\n",
      "Processed batch number: 1760\n",
      "Processed batch number: 1770\n",
      "Processed batch number: 1780\n",
      "Processed batch number: 1790\n",
      "Processed batch number: 1800\n",
      "Processed batch number: 1810\n",
      "Processed batch number: 1820\n",
      "Processed batch number: 1830\n",
      "Processed batch number: 1840\n",
      "Processed batch number: 1850\n",
      "Processed batch number: 1860\n",
      "Processed batch number: 1870\n",
      "Processed batch number: 1880\n",
      "Processed batch number: 1890\n",
      "Processed batch number: 1900\n",
      "Processed batch number: 1910\n",
      "Processed batch number: 1920\n",
      "Processed batch number: 1930\n",
      "Processed batch number: 1940\n",
      "Processed batch number: 1950\n",
      "Processed batch number: 1960\n",
      "Processed batch number: 1970\n",
      "Processed batch number: 1980\n",
      "Processed batch number: 1990\n",
      "Processed batch number: 2000\n",
      "Processed batch number: 2010\n",
      "Processed batch number: 2020\n",
      "Processed batch number: 2030\n",
      "Processed batch number: 2040\n",
      "Processed batch number: 2050\n",
      "Processed batch number: 2060\n",
      "Processed batch number: 2070\n",
      "Processed batch number: 2080\n",
      "Processed batch number: 2090\n",
      "Processed batch number: 2100\n",
      "Processed batch number: 2110\n",
      "Processed batch number: 2120\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "path = 'datasets/fakeddit/images'\n",
    "dataset = 'fakeddit'\n",
    "backbone = 'dinov2_base'\n",
    "out_dir = 'Embeddings'\n",
    "image_files = pd.read_csv('datasets/fakeddit/labels.csv')['id'].tolist()\n",
    "\n",
    "get_embeddings_df(batch_size=batch_size, path=path, dataset_name=dataset, backbone=backbone, directory=out_dir, image_files=image_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02003d44",
   "metadata": {},
   "source": [
    "* **[Recipes5k Dataset](http://www.ub.edu/cvub/recipes5k/)**:\n",
    "\n",
    "The Recipes5k dataset comprises 4,826 recipes featuring images and corresponding ingredient lists, with 3,213 unique ingredients simplified from 1,014 by removing overly-descriptive particles, offering a diverse collection of alternative preparations for each of the 101 food types from Food101, meticulously balanced across training, validation, and test splits. The dataset addresses intra- and inter-class variability, extracted from Yummly with 50 recipes per food type.\n",
    "\n",
    "\n",
    "We'll use the function `get_embeddings_df` to generate the embeddings in `datasets/Recipes5k/images` and store the embeddings in `Embeddings/Recipes5k/Embeddings_Backbone.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40c76d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################  dinov2_base  ##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/datascience/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch number: 10\n",
      "Processed batch number: 20\n",
      "Processed batch number: 30\n",
      "Processed batch number: 40\n",
      "Processed batch number: 50\n",
      "Processed batch number: 60\n",
      "Processed batch number: 70\n",
      "Processed batch number: 80\n",
      "Processed batch number: 90\n",
      "Processed batch number: 100\n",
      "Processed batch number: 110\n",
      "Processed batch number: 120\n",
      "Processed batch number: 130\n",
      "Processed batch number: 140\n",
      "Processed batch number: 150\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "path = 'datasets/Recipes5k/images'\n",
    "dataset = 'Recipes5k'\n",
    "backbone = 'dinov2_base'\n",
    "out_dir = 'Embeddings'\n",
    "image_files = pd.read_csv('datasets/Recipes5k/labels.csv')['image'].tolist()\n",
    "\n",
    "get_embeddings_df(batch_size=batch_size, path=path, dataset_name=dataset, backbone=backbone, directory=out_dir, image_files=image_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b8dc69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_fusion_v0_0_1]",
   "language": "python",
   "name": "conda-env-data_fusion_v0_0_1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
