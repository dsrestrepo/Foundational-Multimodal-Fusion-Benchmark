{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f4a1ecd",
   "metadata": {},
   "source": [
    "### Setup Environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b81fbf-0ee8-498b-bceb-c1558c0bef1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.embeddings import get_embeddings_df\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eea893",
   "metadata": {},
   "source": [
    "## Embeddings Generation\n",
    "\n",
    "* **Batch Size:** Images per batch to convert to embeddings (Adjust depending on your memory)\n",
    "\n",
    "* **Path:** Path to the images\n",
    "\n",
    "* **Output Directory:** Directory to save the embeddings\n",
    "\n",
    "* **Backbone:** Select a backbone from the list of possible backbones:\n",
    "    * 'dinov2_small'\n",
    "    * 'dinov2_base'\n",
    "    * 'dinov2_large'\n",
    "    * 'dinov2_giant'\n",
    "    * 'sam_base'\n",
    "    * 'sam_large'\n",
    "    * 'sam_huge'\n",
    "    * 'clip_base',\n",
    "    * 'clip_large',\n",
    "    * 'convnextv2_tiny'\n",
    "    * 'convnextv2_base'\n",
    "    * 'convnextv2_large'\n",
    "    * 'convnext_tiny'\n",
    "    * 'convnext_small'\n",
    "    * 'convnext_base'\n",
    "    * 'convnext_large'\n",
    "    * 'swin_tiny'\n",
    "    * 'swin_small'\n",
    "    * 'swin_base'\n",
    "    * 'vit_base'\n",
    "    * 'vit_large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "628a2033",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dinov2_small',\n",
       " 'dinov2_base',\n",
       " 'dinov2_large',\n",
       " 'dinov2_giant',\n",
       " 'clip_base',\n",
       " 'clip_large',\n",
       " 'sam_base',\n",
       " 'sam_large',\n",
       " 'sam_huge',\n",
       " 'convnextv2_tiny',\n",
       " 'convnextv2_base',\n",
       " 'convnextv2_large',\n",
       " 'convnext_tiny',\n",
       " 'convnext_small',\n",
       " 'convnext_base',\n",
       " 'convnext_large',\n",
       " 'swin_tiny',\n",
       " 'swin_small',\n",
       " 'swin_base',\n",
       " 'vit_base',\n",
       " 'vit_large']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Foundational Models\n",
    "dino_backbone = ['dinov2_small', 'dinov2_base', 'dinov2_large', 'dinov2_giant']\n",
    "\n",
    "sam_backbone = ['sam_base', 'sam_large', 'sam_huge']\n",
    "\n",
    "clip_backbone = ['clip_base', 'clip_large']\n",
    "\n",
    "# ImageNet:\n",
    "\n",
    "### Convnext\n",
    "convnext_backbone = ['convnextv2_tiny', 'convnextv2_base', 'convnextv2_large'] + ['convnext_tiny', 'convnext_small', 'convnext_base', 'convnext_large']\n",
    "\n",
    "### Swin Transformer\n",
    "swin_transformer_backbone = ['swin_tiny', 'swin_small', 'swin_base']\n",
    "\n",
    "### ViT\n",
    "vit_backbone = ['vit_base', 'vit_large']\n",
    "\n",
    "backbones = dino_backbone + clip_backbone + sam_backbone + convnext_backbone + swin_transformer_backbone + vit_backbone\n",
    "\n",
    "backbones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2853b3",
   "metadata": {},
   "source": [
    "## 1. DAQUAR\n",
    "\n",
    "* **[DAQUAR Dataset](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/vision-and-language/visual-turing-challenge#c7057)**:\n",
    "\n",
    "DAQUAR (Dataset for Question Answering on Real-world images) dataset was created for the purpose of advancing research in visual question answering (VQA). It consists of indoor scene images, each accompanied by sets of questions related to the scene's content. The dataset serves as a benchmark for training and evaluating models in understanding images and answering questions about them.\n",
    "\n",
    "We'll use the function `get_embeddings_df` to generate the embeddings in `datasets/daquar/images` and store the embeddings in `Embeddings/daquar/Embeddings_Backbone.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb6e298",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "path = 'datasets/daquar/images'\n",
    "dataset = 'daquar'\n",
    "backbone = 'dinov2_base'\n",
    "out_dir = 'Embeddings'\n",
    "\n",
    "get_embeddings_df(batch_size=batch_size, path=path, dataset_name=dataset, backbone=backbone, directory=out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9afc23",
   "metadata": {},
   "source": [
    "## 2. COCO-QA\n",
    "\n",
    "* **[COCO-QA Dataset](https://www.cs.toronto.edu/~mren/research/imageqa/data/cocoqa/)**:\n",
    "\n",
    "The COCO-QA (COCO Question-Answering) dataset is designed for the task of visual question-answering. It is a subset of the COCO (Common Objects in Context) dataset, which is a large-scale dataset containing images with object annotations. The COCO-QA dataset extends the COCO dataset by including questions and answers associated with the images. Each image in the COCO-QA dataset is accompanied by a set of questions and corresponding answers.\n",
    "\n",
    "We'll use the function `get_embeddings_df` to generate the embeddings in `datasets/coco-qa/images` and store the embeddings in `Embeddings/coco-qa/Embeddings_Backbone.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7f760e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "path = 'datasets/coco-qa/images'\n",
    "dataset = 'coco-qa'\n",
    "backbone = 'dinov2_base'\n",
    "out_dir = 'Embeddings'\n",
    "\n",
    "get_embeddings_df(batch_size=batch_size, path=path, dataset_name=dataset, backbone=backbone, directory=out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394bdbd2",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3e5971",
   "metadata": {},
   "source": [
    "## 2. Fakeddit\n",
    "\n",
    "* **[Fakeddit Dataset](https://fakeddit.netlify.app/)**:\n",
    "\n",
    "Fakeddit is a large-scale multimodal dataset for fine-grained fake news detection. It consists of over 1 million samples from multiple categories of fake news, including satire, misinformation, and fabricated news. The dataset includes text, images, metadata, and comment data, making it a rich resource for developing and evaluating fake news detection models.\n",
    "\n",
    "We'll use the function `get_embeddings_df` to generate the embeddings in `datasets/fakeddit/images` and store the embeddings in `Embeddings/fakeddit/Embeddings_Backbone.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e145187b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "path = 'datasets/fakeddit/images'\n",
    "dataset = 'fakeddit'\n",
    "backbone = 'dinov2_base'\n",
    "out_dir = 'Embeddings'\n",
    "image_files = pd.read_csv('datasets/fakeddit/labels.csv')['id'].tolist()\n",
    "\n",
    "get_embeddings_df(batch_size=batch_size, path=path, dataset_name=dataset, backbone=backbone, directory=out_dir, image_files=image_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a18d1e",
   "metadata": {},
   "source": [
    "## 4. Recipes5k\n",
    "\n",
    "* **[Recipes5k Dataset](http://www.ub.edu/cvub/recipes5k/)**:\n",
    "\n",
    "The Recipes5k dataset comprises 4,826 recipes featuring images and corresponding ingredient lists, with 3,213 unique ingredients simplified from 1,014 by removing overly-descriptive particles, offering a diverse collection of alternative preparations for each of the 101 food types from Food101, meticulously balanced across training, validation, and test splits. The dataset addresses intra- and inter-class variability, extracted from Yummly with 50 recipes per food type.\n",
    "\n",
    "\n",
    "We'll use the function `get_embeddings_df` to generate the embeddings in `datasets/Recipes5k/images` and store the embeddings in `Embeddings/Recipes5k/Embeddings_Backbone.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "path = 'datasets/Recipes5k/images'\n",
    "dataset = 'Recipes5k'\n",
    "backbone = 'dinov2_base'\n",
    "out_dir = 'Embeddings'\n",
    "image_files = pd.read_csv('datasets/Recipes5k/labels.csv')['image'].tolist()\n",
    "\n",
    "get_embeddings_df(batch_size=batch_size, path=path, dataset_name=dataset, backbone=backbone, directory=out_dir, image_files=image_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae16303f",
   "metadata": {},
   "source": [
    "## 5. BRSET\n",
    "* **[BRSET Dataset](https://physionet.org/content/brazilian-ophthalmological/1.0.0/)**:\n",
    "\n",
    "The Brazilian Multilabel Ophthalmological Dataset (BRSET) stands as a pioneering initiative aimed at bridging the gap in ophthalmological datasets, particularly for under-represented populations in low and medium-income countries. This comprehensive dataset encompasses 16,266 images from 8,524 Brazilian patients, incorporating a wide array of data points including demographics, anatomical parameters of the macula, optic disc, and vessels, along with quality control metrics such as focus, illumination, image field, and artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96005291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################  dinov2_giant  ##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /gpfs/workdir/restrepoda/.cache/torch/hub/main.zip\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_pretrain.pth\" to /gpfs/workdir/restrepoda/.cache/torch/hub/checkpoints/dinov2_vitg14_pretrain.pth\n",
      "100%|██████████| 4.23G/4.23G [00:39<00:00, 115MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch number: 10\n",
      "Processed batch number: 20\n",
      "Processed batch number: 30\n",
      "Processed batch number: 40\n",
      "Processed batch number: 50\n",
      "Processed batch number: 60\n",
      "Processed batch number: 70\n",
      "Processed batch number: 80\n",
      "Processed batch number: 90\n",
      "Processed batch number: 100\n",
      "Processed batch number: 110\n",
      "Processed batch number: 120\n",
      "Processed batch number: 130\n",
      "Processed batch number: 140\n",
      "Processed batch number: 150\n",
      "Processed batch number: 160\n",
      "Processed batch number: 170\n",
      "Processed batch number: 180\n",
      "Processed batch number: 190\n",
      "Processed batch number: 200\n",
      "Processed batch number: 210\n",
      "Processed batch number: 220\n",
      "Processed batch number: 230\n",
      "Processed batch number: 240\n",
      "Processed batch number: 250\n",
      "Processed batch number: 260\n",
      "Processed batch number: 270\n",
      "Processed batch number: 280\n",
      "Processed batch number: 290\n",
      "Processed batch number: 300\n",
      "Processed batch number: 310\n",
      "Processed batch number: 320\n",
      "Processed batch number: 330\n",
      "Processed batch number: 340\n",
      "Processed batch number: 350\n",
      "Processed batch number: 360\n",
      "Processed batch number: 370\n",
      "Processed batch number: 380\n",
      "Processed batch number: 390\n",
      "Processed batch number: 400\n",
      "Processed batch number: 410\n",
      "Processed batch number: 420\n",
      "Processed batch number: 430\n",
      "Processed batch number: 440\n",
      "Processed batch number: 450\n",
      "Processed batch number: 460\n",
      "Processed batch number: 470\n",
      "Processed batch number: 480\n",
      "Processed batch number: 490\n",
      "Processed batch number: 500\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "#path = 'datasets/brset/images'\n",
    "path = '/gpfs/workdir/restrepoda/datasets/BRSET/brset/images'\n",
    "dataset = 'brset'\n",
    "backbone = 'dinov2_giant'\n",
    "out_dir = 'Embeddings'\n",
    "\n",
    "get_embeddings_df(batch_size=batch_size, path=path, dataset_name=dataset, backbone=backbone, directory=out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8696bf6b-a703-4382-87a3-df609661f829",
   "metadata": {},
   "source": [
    "## 5. mBRSET\n",
    "* **[mBRSET Dataset](https://physionet.org/content/mbrset/1.0/)**:\n",
    "\n",
    "The Mobile Brazilian Multilabel Ophthalmological Dataset (mBRSET) stands as a pioneering initiative aimed at bridging the gap in ophthalmological datasets using mobile cameras, particularly for under-represented populations in low and medium-income countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7701b708-16c3-4920-b857-17b665ba5c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################  dinov2_giant  ##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /gpfs/workdir/restrepoda/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch number: 10\n",
      "Processed batch number: 20\n",
      "Processed batch number: 30\n",
      "Processed batch number: 40\n",
      "Processed batch number: 50\n",
      "Processed batch number: 60\n",
      "Processed batch number: 70\n",
      "Processed batch number: 80\n",
      "Processed batch number: 90\n",
      "Processed batch number: 100\n",
      "Processed batch number: 110\n",
      "Processed batch number: 120\n",
      "Processed batch number: 130\n",
      "Processed batch number: 140\n",
      "Processed batch number: 150\n",
      "Processed batch number: 160\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "path = 'datasets/brset/images'\n",
    "path = '/gpfs/workdir/restrepoda/datasets/mBRSET/mbrset/images'\n",
    "dataset = 'mbrset'\n",
    "backbone = 'dinov2_giant'\n",
    "out_dir = 'Embeddings'\n",
    "\n",
    "get_embeddings_df(batch_size=batch_size, path=path, dataset_name=dataset, backbone=backbone, directory=out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e24a09",
   "metadata": {},
   "source": [
    "### 6. HAM10000 dataset\n",
    "\n",
    "* [HAM10000 dataset](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T)\n",
    "\n",
    "The MNIST: HAM10000 dataset is a large collection of dermatoscopic images from different populations, acquired and stored by the Department of Dermatology at the Medical University of Vienna, Austria. It consists of 10,015 dermatoscopic images which can serve as a training set for academic machine learning purposes in tasks like skin lesion analysis and classification, specifically focusing on the detection of melanoma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1497f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "path = 'datasets/ham10000/images'\n",
    "dataset = 'ham10000'\n",
    "backbone = 'dinov2_base'\n",
    "out_dir = 'Embeddings'\n",
    "\n",
    "get_embeddings_df(batch_size=batch_size, path=path, dataset_name=dataset, backbone=backbone, directory=out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd2288a",
   "metadata": {},
   "source": [
    "### 7. Colombian Multimodal Satellite dataset\n",
    "* **[A Multi-Modal Satellite Imagery Dataset for Public Health Analysis in Colombia](https://physionet.org/content/multimodal-satellite-data/1.0.0/)**:\n",
    "\n",
    "The Multi-Modal Satellite Imagery Dataset in Colombia integrates economic, demographic, meteorological, and epidemiological data. It comprises 12,636 high-quality satellite images from 81 municipalities between 2016 and 2018, with minimal cloud cover. Its applications include deforestation monitoring, education indices forecasting, water quality assessment, extreme climatic event tracking, epidemic illness addressing, and precision agriculture optimization. We'll use it shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71951501",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "path = 'datasets/satellitedata/images'\n",
    "dataset = 'satellitedata'\n",
    "backbone = 'dinov2_base'\n",
    "out_dir = 'Embeddings'\n",
    "\n",
    "get_embeddings_df(batch_size=batch_size, path=path, dataset_name=dataset, backbone=backbone, directory=out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13033069",
   "metadata": {},
   "source": [
    "## 8. MIMIC CXR\n",
    "* **[MIMIC CXR](https://physionet.org/content/mimic-cxr/2.0.0/#files-panel)**:\n",
    "\n",
    "The MIMIC-CXR (Medical Information Mart for Intensive Care, Chest X-Ray) dataset is a large, publicly available collection of chest radiographs with associated radiology reports. It was developed by the MIT Lab for Computational Physiology and provides an extensive resource for training and evaluating machine learning models in the field of medical imaging, particularly in automated radiograph interpretation and natural language processing for clinical narratives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b240cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "path = 'datasets/mimic/images'\n",
    "dataset = 'mimic'\n",
    "backbone = 'dinov2_base'\n",
    "out_dir = 'Embeddings'\n",
    "\n",
    "get_embeddings_df(batch_size=batch_size, path=path, dataset_name=dataset, backbone=backbone, directory=out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9ad2e1",
   "metadata": {},
   "source": [
    "# 9. Joslin Center Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf34902",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "path = 'datasets/joslin/images'\n",
    "dataset = 'joslin'\n",
    "backbone = 'dinov2_base'\n",
    "out_dir = 'Embeddings'\n",
    "device = \"cuda\"\n",
    "\n",
    "get_embeddings_df(batch_size=batch_size, path=path, dataset_name=dataset, backbone=backbone, directory=out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d01954-2f62-42fc-a963-424f265bd059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
