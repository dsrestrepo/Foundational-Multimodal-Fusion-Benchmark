{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6753a915",
   "metadata": {},
   "source": [
    "### Setup Environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07a910e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.embeddings import get_embeddings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed6dd64",
   "metadata": {},
   "source": [
    "## Embeddings Generation\n",
    "\n",
    "* **Batch Size:** Images per batch to convert to embeddings (Adjust depending on your memory)\n",
    "\n",
    "* **Path:** Path to the images\n",
    "\n",
    "* **Output Directory:** Directory to save the embeddings\n",
    "\n",
    "* **Backbone:** Select a backbone from the list of possible backbones:\n",
    "    * 'dinov2_small'\n",
    "    * 'dinov2_base'\n",
    "    * 'dinov2_large'\n",
    "    * 'dinov2_giant'\n",
    "    * 'sam_base'\n",
    "    * 'sam_large'\n",
    "    * 'sam_huge'\n",
    "    * 'clip_base',\n",
    "    * 'clip_large',\n",
    "    * 'convnextv2_tiny'\n",
    "    * 'convnextv2_base'\n",
    "    * 'convnextv2_large'\n",
    "    * 'convnext_tiny'\n",
    "    * 'convnext_small'\n",
    "    * 'convnext_base'\n",
    "    * 'convnext_large'\n",
    "    * 'swin_tiny'\n",
    "    * 'swin_small'\n",
    "    * 'swin_base'\n",
    "    * 'vit_base'\n",
    "    * 'vit_large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e0b6d2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dinov2_small',\n",
       " 'dinov2_base',\n",
       " 'dinov2_large',\n",
       " 'dinov2_giant',\n",
       " 'clip_base',\n",
       " 'clip_large',\n",
       " 'sam_base',\n",
       " 'sam_large',\n",
       " 'sam_huge',\n",
       " 'convnextv2_tiny',\n",
       " 'convnextv2_base',\n",
       " 'convnextv2_large',\n",
       " 'convnext_tiny',\n",
       " 'convnext_small',\n",
       " 'convnext_base',\n",
       " 'convnext_large',\n",
       " 'swin_tiny',\n",
       " 'swin_small',\n",
       " 'swin_base',\n",
       " 'vit_base',\n",
       " 'vit_large']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Foundational Models\n",
    "dino_backbone = ['dinov2_small', 'dinov2_base', 'dinov2_large', 'dinov2_giant']\n",
    "\n",
    "sam_backbone = ['sam_base', 'sam_large', 'sam_huge']\n",
    "\n",
    "clip_backbone = ['clip_base', 'clip_large']\n",
    "\n",
    "# ImageNet:\n",
    "\n",
    "### Convnext\n",
    "convnext_backbone = ['convnextv2_tiny', 'convnextv2_base', 'convnextv2_large'] + ['convnext_tiny', 'convnext_small', 'convnext_base', 'convnext_large']\n",
    "\n",
    "### Swin Transformer\n",
    "swin_transformer_backbone = ['swin_tiny', 'swin_small', 'swin_base']\n",
    "\n",
    "### ViT\n",
    "vit_backbone = ['vit_base', 'vit_large']\n",
    "\n",
    "backbones = dino_backbone + clip_backbone + sam_backbone + convnext_backbone + swin_transformer_backbone + vit_backbone\n",
    "\n",
    "backbones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bf6d60",
   "metadata": {},
   "source": [
    "### Satellite Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb52b578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f3362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0574fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5908247",
   "metadata": {},
   "source": [
    "* **[DAQUAR Dataset](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/vision-and-language/visual-turing-challenge#c7057)**:\n",
    "\n",
    "DAQUAR (Dataset for Question Answering on Real-world images) dataset was created for the purpose of advancing research in visual question answering (VQA). It consists of indoor scene images, each accompanied by sets of questions related to the scene's content. The dataset serves as a benchmark for training and evaluating models in understanding images and answering questions about them.\n",
    "\n",
    "This dataset can be downloaded from the following [link](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/vision-and-language/visual-turing-challenge#c7057). Or you can download the dataset using the function `get_daquar_dataset`.\n",
    "\n",
    "Once you have the dataset, use the function `preprocess_daquar_dataset` to proprocess the train and test set, and generate the `labes.csv` file.\n",
    "\n",
    "These functions will generate a dataset with the structure:\n",
    "\n",
    "* output_dir/\n",
    "    * labels.csv\n",
    "    * test.txt\n",
    "    * train.txt\n",
    "    * images/\n",
    "        * image1.png\n",
    "        * image2.png\n",
    "        * image3.png\n",
    "        \n",
    "        ...\n",
    "        \n",
    "        * imagen.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46c1396d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images downloaded and uncompressed successfully.\n",
      "Labels downloaded successfully.\n",
      "Preprocessed data saved to datasets/daquar/labels.csv\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'datasets/daquar/'\n",
    "get_daquar_dataset(output_dir)\n",
    "preprocess_daquar_dataset(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bfc680",
   "metadata": {},
   "source": [
    "* **[COCO-QA Dataset](https://www.cs.toronto.edu/~mren/research/imageqa/data/cocoqa/)**:\n",
    "\n",
    "The COCO-QA (COCO Question-Answering) dataset is designed for the task of visual question-answering. It is a subset of the COCO (Common Objects in Context) dataset, which is a large-scale dataset containing images with object annotations. The COCO-QA dataset extends the COCO dataset by including questions and answers associated with the images. Each image in the COCO-QA dataset is accompanied by a set of questions and corresponding answers.\n",
    "\n",
    "You can use the `get_cocoqa_dataset` Function to download the dataset.\n",
    "\n",
    "Example usage of the function:\n",
    "\n",
    "`get_cocoqa_dataset(output_dir=\"datasets/coco-qa/\")`\n",
    "\n",
    "Also run the function to preprocess the dataset:\n",
    "\n",
    "`process_cocoqa_data(output_dir=\"datasets/coco-qa/\")`\n",
    "\n",
    "After executing these functions, you will have the following structure in the \"datasets/coco-qa/\" directory:\n",
    "\n",
    "* datasets/coco-qa/\n",
    "    * labels.csv\n",
    "    * train/\n",
    "    * test/\n",
    "    * images/\n",
    "        * image1.png\n",
    "        * image2.png\n",
    "        * image3.png\n",
    "        \n",
    "        ...\n",
    "        \n",
    "        * imagen.png "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c9f3be",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "283e6666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO-QA dataset downloaded and uncompressed successfully.\n",
      "COCO images downloaded and uncompressed successfully.\n",
      "Train and test dataframes saved successfully.\n",
      "Combined dataframe saved successfully.\n",
      "Images removed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "output_dir = 'datasets/coco-qa/'\n",
    "get_cocoqa_dataset(output_dir)\n",
    "process_cocoqa_data(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410535e3",
   "metadata": {},
   "source": [
    "* **[Fakeddit Dataset](https://fakeddit.netlify.app/)**:\n",
    "\n",
    "Fakeddit is a large-scale multimodal dataset for fine-grained fake news detection. It consists of over 1 million samples from multiple categories of fake news, including satire, misinformation, and fabricated news. The dataset includes text, images, metadata, and comment data, making it a rich resource for developing and evaluating fake news detection models.\n",
    "\n",
    "You can use the se the function `download_fakeddit_files` to download the metadata, and the function `download_full_set_images`to get the full set of Images. \n",
    "\n",
    "Since the full set of images contains 1M images, we'll provide a function to generate a subset, to run the experiments with less resources. Use the function `create_stratified_subset_fakeddit`. This function will generate a `labels.csv` file with the subset.\n",
    "\n",
    "You can also use the `download_images_from_file` to download the images from an specific file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b5a1246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1cjY6HsHaSZuLVHywIxD5xQqng33J5S2b\n",
      "From (redirected): https://drive.google.com/uc?id=1cjY6HsHaSZuLVHywIxD5xQqng33J5S2b&confirm=t&uuid=25f70429-0d7f-4575-a219-133d8d39a23b\n",
      "To: /home/datascience/Data Fusion/datasets/fakeddit/Images.tar.bz2\n",
      "100%|██████████| 114G/114G [16:12<00:00, 117MB/s]    \n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "output_dir = 'datasets/fakeddit/'\n",
    "\n",
    "# Get Metadata:\n",
    "download_fakeddit_files(output_dir)\n",
    "\n",
    "# Get Images (Due to possible API changes, we recommend this method):\n",
    "download_full_set_images(output_dir)\n",
    "\n",
    "# Random subset:\n",
    "subset_size = 0.1  # 10% subset size\n",
    "create_stratified_subset_fakeddit(output_dir, subset_size)\n",
    "\n",
    "#download_images_from_file(output_dir, 'labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c2168b",
   "metadata": {},
   "source": [
    "* **[Recipes5k Dataset](http://www.ub.edu/cvub/recipes5k/)**:\n",
    "\n",
    "The Recipes5k dataset comprises 4,826 recipes featuring images and corresponding ingredient lists, with 3,213 unique ingredients simplified from 1,014 by removing overly-descriptive particles, offering a diverse collection of alternative preparations for each of the 101 food types from Food101, meticulously balanced across training, validation, and test splits. The dataset addresses intra- and inter-class variability, extracted from Yummly with 50 recipes per food type.\n",
    "\n",
    "You can use the se the function `download_recipes5k_dataset` to download the dataset. Use the function `preprocess_recipes5k` to preprocess the dataset. These function will generate the following structure:\n",
    "\n",
    "* preprocess_recipes5k\n",
    "    * labels.csv\n",
    "    * Images/\n",
    "        * class_1/\n",
    "            * img_1\n",
    "            * img_2\n",
    "            ...\n",
    "        * class_2/\n",
    "            * img_1\n",
    "            * img_2\n",
    "            ...\n",
    "        ...\n",
    "        * class_n/\n",
    "            * img_1\n",
    "            * img_2\n",
    "            ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4d7f36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# The function generates the directory 'Recipes5k' by default, so you don't have to specify that.\n",
    "output_dir = 'datasets/'\n",
    "download_recipes5k_dataset(output_dir)\n",
    "preprocess_recipes5k('datasets/Recipes5k/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a19ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_fusion_v0_0_1]",
   "language": "python",
   "name": "conda-env-data_fusion_v0_0_1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
